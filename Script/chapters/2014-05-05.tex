\section[Practice: Probability Refresher II]{Practice: Probability Refresher II \iftoggle{showdates}{\small{\textit{2014-05-05}}}{}}
\subsection*{Exercise 1}

% This should be changed to a simpler form of roulette
There are different types of roulette, we will only deal with the so-called 
``European'' one, which features only one $0$, no $00$.

The sample space $\Omega$ is $\Omega=\left\{0, 1, 2, 3, ..., 36\right\}$. The event space is $\mathcal{P} \left( \Omega \right) \setminus \Omega$, the number of events $2^{|\Omega|} = 2^{37} = 2^{10} * 2^{10} * 2^{10} * 2^7 \approx 128,000,000,000 (128\ Bio)$.

The expected values are:
\begin{align*}
E(even) &= 18 \cdot \frac{18}{37} - 18 \cdot \frac{19}{37} = -0.49\\
E(even\ numbers) &= \left(35 \cdot \frac{1}{37} - 1 \cdot \frac{36}{37} \right) * 18 = -0.49
\end{align*}
So basically both variants are the same. 

However, in European roulette there usually exists the so-called ``en prison''-rule. This rule freezes (``imprisons'') the stakes made on the small bets (``odd'', ``even'', ``red'', ``black'', ``high'', ``low'') after a $0$ until the specific bet was fulfilled twice in a row. When it came up twice in a row, you are allowed to get your stakes back. Alternatively, if you want to play with your bets, you can keep half of them instead of letting them being frozen, losing the other half. This modifies the expected value:

\begin{align*}
E(even, 0, freeze) &= 18 \cdot \frac{18}{37} - 18 \cdot \frac{18}{37} - 0 \cdot \frac{1}{37} = 0  \\
E(even, 0, return) &= 18 \cdot \frac{18}{37} - 18 \cdot \frac{18}{37} - 9 \cdot \frac{1}{37} = -0.24  \\
E(even, 0) &= \frac{1}{2} \left( E(even, 0, return) + E(even, 0, freeze) \right) = -0.12 \\
E(even) &= \frac{\frac{1}{37} E(even, 0) + \frac{36}{37} E(even, \neg 0)}{2} = -0.48
\end{align*}

With the ``en prison'' rule betting on ``even'' is therefore better than betting on all even numbers.

{ }

The Bank makes money because of the $0$: For calculating the odds they assume not 37 but 36 numbers.


\subsection*{Exercise 2}
\setcounter{equation}{0}
The union-bound can be proved by induction.

\textbf{Induction assumption}
\begin{align}
& & P\left(\bigcup\limits_{i=1}^{n}E_i\right) & \leq \sum\limits_{i=1}^{n}(P(E_i)) & &
\end{align}

\textbf{Base case}

For $n=1$ we have
\begin{align}
& & P\left(\bigcup\limits_{i=1}^{1}E_i\right) & \leq \sum\limits_{i=1}^{1}(P(E_i)) & & \\
\Leftrightarrow & & P(E_1) & \leq P(E_1) & & 
\end{align}

\textbf{Inductive Step}

We know that $P(A \cup B) = P(A) + P(B) - P(A \cap B)$, so it follows that:
\begin{align}
& & P\left(\bigcup\limits_{i=1}^{n+1}E_i\right) & = P\left(\bigcup\limits_{i=1}^{n}E_i\right) + P(E_{n+1}) - P\left(\bigcup\limits_{i=1}^{n}E_i \cap E_{n+1}\right) & & \\
& & \mbox{We use the induction assumption:} & & & \\
& & & \leq \sum\limits_{i=1}^{n}(P(E_i)) + P(E_{n+1}) = \sum\limits_{i=1}^{n+1}(P(E_i)) & & 
\end{align}

%Now since $P\left(\bigcup_{i=1}^{n}(E_i) \cap E_{n+1}\right)$ is a probability, by the first axiom of probability we know that it is bigger or equal to zero:
%\begin{align}
 %P\left(\bigcup_{i=1}^{n}(E_i) \cap E_{n+1}\right) \geq 0 
%\end{align}
%
%By adding it to both sides of the inequality we get:
%\begin{align}
%& & P(\bigcup_{i=1}^{n+1}(E_i)) + P(\bigcup_{i=1}^{n}(E_i) \cap E_{n+1}) & = P(\bigcup_{i=1}^{n}(E_i)) + P(E_{n+1})  & &
%\end{align}

%And therefore we know:
%\begin{align}
%& & P(\bigcup_{i=1}^{n+1}(E_i)) \leq P(\bigcup_{i=1}^{n}(E_i)) + P(E_{n+1})  & &
%\end{align}
%
%This is equvialent to:
%\begin{align}
%& & P(\bigcup_{i=1}^{n+1}(E_i)) \leq \sum_{i=1}^{n}(P(E_i) + P(E_{n+1})) & = \sum_{i=1}^{n+1}(P(E_i)) & &
%\end{align}
%\setcounter{equation}{0}

\subsection*{Exercise 3}
\begin{align*}
P(A|B) & = \frac{P(B|A)P(A)}{P(B)} \\
P(\neg A|B) & = \frac{P(B|\neg A)P(\neg A)}{P(B)}
\end{align*}

\begin{align*}
\frac{P(A|B)}{P(\neg A|B)} = \frac{P(B|A)}{P(B|\neg A)} \frac{P(A)}{P(\neg A)}
\end{align*}

This is like:
\begin{align*}
Posterior\ Odds\ =\ Likelihood\ Ratio\ \cdot\ Prior Odds
\end{align*}
We can say this is updating your beliefs with new information.


\subsection*{Exercise 4}
We take Bayes' rule to calculate $P(\mbox{Blue Car}|\mbox{Testified by Witness})$
\begin{align*}
P(B|T) = \frac{P(T|B)P(B)}{P(T)}
\end{align*}

We know that $P(T|B) = 0.8$ and $P(B) = 0.15$, so we only have to find out $P(T)$. Therefore we marginalize over B:
\begin{align*}
\sum_{B}{P(T|B)P(B)} = 0.8 \cdot 0.15 + 0.2 \cdot 0.85 = 0.29
\end{align*}

Now we can take our values and calculate the correct answer:
\begin{align*}
P(B|T) = \frac{P(T|B)P(B)}{P(T)} = \frac{0.8 \cdot 0.15}{0.29} = 0.414
\end{align*}

When testing a large group of people, the common mistake becomes obvious: they take the probability that the witness correctly identified the color, which is 80 \%. This is because they neglect the base rate (\textit{base rate neglect}).


\subsection*{Exercise 5}
We take Bayes' rule to calculate $P(\mbox{Cancer}|\mbox{Positive Test})$
\begin{align*}
P(C|P) = \frac{P(P|C)P(C)}{P(P)}
\end{align*}

We know that $P(P|C) = 0.5$ and $P(C) = 0.003$, so we only have to find out $P(P)$. Therefore we marginalize over C:
\begin{align*}
\sum_{C}{P(P|C)P(C)} = 0.5 \cdot 0.003 + 0.03 \cdot 0.997 = 0.0314
\end{align*}

Now we can take our values and calculate the correct answer:
\begin{align*}
P(C|P) = \frac{P(P|C)P(C)}{P(P)} = \frac{0.5 \cdot 0.003}{0.0314} = 0.048
\end{align*}

As we can see, only 4.8 \% of the people being tested positively actually have cancer. Nevertheless screening is a good thing because out of 1000 detections still 50 have cancer and those can be treated which probably makes up for the disadvantage of having 950 false alarms.  

\subsection*{Exercise 6}
\textbf{$P(P, C)$}
\begin{tabular}{ r|c|c|l }
\multicolumn{1}{r}{}
      &  \multicolumn{1}{c}{Cancer}
               &  \multicolumn{1}{c}{$\neg$ Cancer} \\
               \cline{2-3}
Positive Test  & $\frac{15}{10,000}$ & $\frac{  300}{10,000}$     & $\frac{  315}{10,000}$\\ 
               \cline{2-3}
Negative Test  & $\frac{15}{10,000}$ & $\frac{9,670}{10,000}$     & $\frac{9,685}{10,000}$\\
               \cline{2-3}
\multicolumn{1}{r}{} 
      & \multicolumn{1}{c}{$\frac{30}{10,000}$} 
              & \multicolumn{1}{c}{$\frac{9,970}{10,000}$}
                      & \multicolumn{1}{c}{$\frac{10,000}{10,000}$}\\
\end{tabular}

\textbf{$P(P|C)$}
\begin{tabular}{ r|c|c|l }
\multicolumn{1}{r}{}
      &  \multicolumn{1}{c}{Cancer}
               &  \multicolumn{1}{c}{$\neg$ Cancer} \\
               \cline{2-3}
Positive Test  & $\frac{15}{30}$ & $\frac{ 300}{9,970}$     & \\ 
               \cline{2-3}
Negative Test  & $\frac{15}{30}$ & $\frac{9,670}{9,970}$     & \\
               \cline{2-3}
\multicolumn{1}{r}{} 
      & \multicolumn{1}{c}{$1$} 
              & \multicolumn{1}{c}{$1$} 
                      & \multicolumn{1}{c}{}\\
\end{tabular}

\textbf{$P(C|P)$}
\begin{tabular}{ r|c|c|l }
\multicolumn{1}{r}{}
      &  \multicolumn{1}{c}{Cancer}
               &  \multicolumn{1}{c}{$\neg$ Cancer} \\
               \cline{2-3}
Positive Test  & $\frac{15}{300 + 15}$   & $1 - \frac{15}{300+15}$  & $1$\\ 
               \cline{2-3}
Negative Test  & $\frac{15}{15 + 9,670}$ & $\frac{9,670}{15+9,970}$ & $1$\\
               \cline{2-3}
\multicolumn{1}{r}{} 
      & \multicolumn{1}{c}{} 
              & \multicolumn{1}{c}{} 
                      & \multicolumn{1}{c}{}\\
\end{tabular}

\textbf{Assume independence $P(C, P)$}

$P(C|P) = P(C)$

\begin{tabular}{ r|c|c|l }
\multicolumn{1}{r}{}
      &  \multicolumn{1}{c}{Cancer}
               &  \multicolumn{1}{c}{$\neg$ Cancer} \\
               \cline{2-3}
Positive Test  & $\frac{15+300}{10,000}\cdot\frac{15+15}{10,000}$   & $\frac{15+300}{10,000}\cdot\frac{300+9,670}{10,000}$  & $\frac{15+300}{10,000}$\\ 
               \cline{2-3}
Negative Test  & $\frac{15+9,670}{10,000}\cdot\frac{15+15}{10,000}$ & $\frac{15+9,670}{10,000}\cdot\frac{300+9,670}{10,000}$ & $\frac{15+9,670}{10,000}$\\
               \cline{2-3}
\multicolumn{1}{r}{} 
      & \multicolumn{1}{c}{} 
              & \multicolumn{1}{c}{} 
                      & \multicolumn{1}{c}{}\\
\end{tabular}


%\begin{align*}
%& P(P) = \sum_{C}{P(P|C) \cdot P(C)} = 0.5 \cdot 0.003 + 0.03 \cdot 0.997 = 0.0314\\
%& P(\neg P) = \sum_{C}{P(\neg P|C) \cdot P(C)} = 0.5 \cdot 0.003 + 0.97 \cdot 0.997 = 0.9686\\ 
%& P(P|C) = \frac{P(P,C)}{P(C)} = \frac{0.0015}{0.003} = 0.5 \\
%& P(P|\neg C) = \frac{P(P,\neg C)}{P(\neg C)} = \frac{0.0299}{0.997} = 0.03 \\
%& P(\neg P|C) = \frac{P(\neg P,C)}{P(C)} = \frac{0.0015}{0.003} = 0.5 \\
%& P(\neg P|\neg C) = \frac{P(\neg P,\neg C)}{P(\neg C)} = \frac{0.9671}{0.997}= 0.97\\
%& P(C|P) = \frac{P(C,P)}{P(P)} = \frac{0.0015}{0.0314} = 0.048 \\
%& P(C|\neg P) = \frac{P(C,\neg P)}{P(\neg P)} = \frac{0.0015}{0.9686} = 0.002 \\
%& P(\neg C|P) = \frac{P(\neg C,P)}{P(P)} = \frac{0.0299}{0.0314} = 0.952 \\
%& P(\neg C|\neg P) = \frac{P(\neg C,\neg P)}{P(\neg P)} = \frac{0.9671}{0.9686}= 0.998 
%\end{align*}
%
%If the events were independent but $P(C)=0.003$, $P(\neg C)=0.997$, $P(P)=0.0314$ and $P(\neg P)=0.9686$ stay as they are, we would get:
%
%\begin{tabular}{ r|c|c|l }
%\multicolumn{1}{r}{}
 %&  \multicolumn{1}{c}{Cancer}
 %& \multicolumn{1}{c}{$\neg$ Cancer} \\
%\cline{2-3}
%Positive Test & 0.009\% & 3.131\% & 3.14\%\\ 
%\cline{2-3}
%Negative Test & 0.291\% & 96.569\% & 96.86\% \\
%\cline{2-3}
%\multicolumn{1}{r}{}
 %&  \multicolumn{1}{c}{0.3\%}
 %& \multicolumn{1}{c}{99.7\%}  & \multicolumn{1}{c}{100\%}\\
%\end{tabular}
%\newpage
%The conditional probabilities change as follows:
%\begin{align*}
%& P(P|C) = \frac{P(P,C)}{P(C)} = \frac{0.00009}{0.003} = 0.03 \\
%& P(P|\neg C) = \frac{P(P,\neg C)}{P(\neg C)} = \frac{0.03131}{0.997} = 0.0314 \\
%& P(\neg P|C) = \frac{P(\neg P,C)}{P(C)} = \frac{0.00291}{0.003} = 0.97 \\
%& P(\neg P|\neg C) = \frac{P(\neg P,\neg C)}{P(\neg C)} = \frac{0.96569}{0.997}= 0.9686\\
%& P(C|P) = \frac{P(C,P)}{P(P)} = \frac{0.00009}{0.0314} = 0.00287 \\
%& P(C|\neg P) = \frac{P(C,\neg P)}{P(\neg P)} = \frac{0.00291}{0.9686} = 0.003 \\
%& P(\neg C|P) = \frac{P(\neg C,P)}{P(P)} = \frac{0.03131}{0.0314} = 0.99713 \\
%& P(\neg C|\neg P) = \frac{P(\neg C,\neg P)}{P(\neg P)} = \frac{0.96569}{0.9686}= 0.997 
%\end{align*}

\subsection*{Exercise 7}
As the probability for each door to contain a car is equal we know that our chance to win is given by $P(\mbox{Win}) = \frac{1} 3$\\
Further the probability that the Quiz-master chooses one of the remaining doors is
$P(\mbox{Open}) = \frac{1} 2$\\
The Quiz-master would not open the door which contains the car so: \\
$P(\mbox{notOpen}|\mbox{Win}) = 1$\\
Now if calculate the probability of the car being behind the not opened door using Bayes' we get:
\begin{align*}
& & P(Win|notOpen) & = \frac{P(notOpen|Win) \cdot P(Win)}{P(notOpen)} & = \frac{1 \cdot \frac{1}{3}}{\frac{1}{2}} &= \frac{2}{3}& &
\end{align*}

Therefore the probability that the car is behind the door that the Quiz-master did not open is $\frac{2}{3}$ while the probability of our first chosen door still is $\frac{1}{3}$. So we would double our chances to win by changing our choice.