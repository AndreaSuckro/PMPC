\section[Frequentist Inference Examples]{Frequentist Inference Examples \iftoggle{showdates}{\small{\textit{2014-05-26}}}{}}

\subsection{Bayesian Inference for Thumbtacks}
Data: 01110101101 Binary Strings
RV: $x_1....x_n =: X$

$P(X|q) = \prod\limits_{i=1}^n{q^{x_i}\left(1-q\right)^{1-x_i}}=\underbrace{q^h\left(1-q\right)^t}_\text{Bernoulli}$

h: \#heads
t: \#tails

$p(q|X) = \frac{P(X|q)\overbrace{p(q)}^\text{prior}}{\int_0^1 P(X|q)p(q) dq}$

prior: what is it? q: parameter of coin, or my belief about it

Non-Bayesian argument: different-people have different beliefs $\rightarrow$ subjective.

but: if you have lots of data the results will converge to $p(q|X)$

as a result we can also use: 

convenient choice: beta distribution

$p(q) = \frac{q^{\alpha-1}(1-q)^{\beta-1}}{\int_0^1 q^{\alpha-1}(1-q)^{\beta-1}dq} = \frac{q^{\alpha-1}(1-q)^{\beta-1}}{B(\alpha,\beta)}$

\begin{align*}
p(q|X) &\propto q^h(1-q)^t q^{\alpha-1}(1-q)^{\beta-1} = \underbrace{q^{h+alpha-1}(1-q)^{t+\beta-1}}_\text{new $\beta$-distribution} \\
\alpha_n &= h+\alpha \\
\beta_n &= t+\beta \\
\Rightarrow p(q|X) &= \frac{1}{B(\alpha_n, \beta_n)}\cdot q^{\alpha_n-1}(1-q)^{\beta_n-1}
\end{align*}

If prior and posterior have the same form, this is called conjugate.

--- graph from notes

\subsection{Map estimate (maximum a posteriori)}
In order to find the maximum posteriori term we need to calculate the first derivative of $p(q|X)$. That seems quite hard but we can reduce the problem by ignoring the normalization term: $\frac{1}{B(\alpha_n, \beta_n)}$ (since it is independent from $q$) and taking the logarithm of the numerator, since this does not change the location of any maxima. By this we just have to maximize $\log(q^{\alpha_n-1}(1-q)^\beta_n-1)$.

\begin{align*}
\log{\left(q^{\alpha_n-1}(1-q)^{\beta_n-1}\right)} &= (\alpha_n-1)\log \est{q} + (\beta_n-1)\log (1-\est{q}) \\
\frac{\partial \left(\left(\alpha_n-1\right)\log \est{q} + \left(\beta_n-1\right)\log \left(1-\est{q}\right)\right)}{\partial \est{q}} &= \frac{\alpha_n-1}{\est{q}} - \frac{\beta_n-1}{1-\est{q}} = 0 \\
\frac{\alpha_n-1}{\est{q}} = \frac{\beta_n-1}{1-\est{q}} &\Leftrightarrow \frac{1-\est{q}}{\est{q}} = \frac{\beta_n-1}{\alpha_n-1} \\
\frac{1}{\est{q}} = \frac{\beta_n-1}{\alpha_n-1}+1 &= \frac{\beta_n-1}{\alpha_n-1} + \frac{\alpha_n-1}{\alpha_n-1} = \frac{\beta_n+\alpha_n-2}{\alpha_n-1} \\
\est{q} = \frac{\alpha_n-1}{\beta_n+\alpha_n-2} &= \frac{\alpha+h-1}{\alpha+\beta+h+t-2}
\end{align*}

\sidenote[.4]{The approach to use Bayesian statistics with a prior that does not assume or put in any information is called 'Objective Bayes'. It is somehow the middle ground between the two opposing camps.}

For $\alpha = \beta =1$ we get what we have already suspected before $\est{q}=\frac{h}{h+t}$. Note that $\alpha$ and $\beta$ are pseudo-counters. They represent data points you have not seen but believe to be realistic. This a way to put your prior belief about the problem in the model but is also dangerous. If you have a strong belief in a hypothesis it will need more and more data to prove in the limit that you are wrong.

\subsection[Null Hypothesis Significance Testing (NHST)]{NHST Null Hypothesis Significance Testing}
In the previous section we examined how Bayesian people tackle the problem of finding a good model for a problem.
$p(q)$ makes no sense: q is not a random variable but unknown

$p(q|X) \propto p(q)p(X|q)$ also makes no sense: q is not a distribution

Example: Can someone discriminate between coke and pepsi?

Start with null hypothesis: Subjects can't discriminate: $q = \frac{1}{2}, n = 25$.

Measure H (\# ``Heads'', correct answers).

\begin{align*}
P(H=h|q) = \underbrace{ \binom{n}{h} \overbrace{q^h (1-q)^{n-h}}^\text{Bernoulli} }_\text{Binomial distribution}
\end{align*}
Introduce criterion: subjects can discard if they get >20 correct answers. Then you can reject the null hypothesis.

--- some gauss distribution from notes

\begin{align*}
P\left(H=h|q\frac{1}{2},n=25\right) \\
P\left(H>20|q\frac{1}{2},n=25\right) \mbox{ (p-Value)}
\end{align*}

$\alpha$ is the signal level $\rightarrow$ type I error rate that's acceptable (usually $5\%$). This is the probability that you say there is an effect even if there is none.

say $q = \frac{4}{5}$

--- another plot from notes

type II error $\beta \approx \frac{3}{4}$

tradeoff between $\alpha$ and $\beta$: ``easier'' for $q \rightarrow \frac{1}{2}$ for $n$ big: the power is $1-\beta$.