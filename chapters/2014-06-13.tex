\documentclass[../main/Notes.tex]{subfiles}
\begin{document}

\section[Signal Detection Theory I]{Signal Detection Theory I \iftoggle{showdates}{\small{\textit{2014-06-13}}}{}}

\subsection{Detection tasks}

Detection tasks are simple yes/no response tasks. Yes and no corresponds most of the time to the existence or absence of a signal. The difficulty of such a task stems from the fact that there is always noise in the system, so we can't be sure whether or not the given answer is correct.

\subsubsection{Examples}
\begin{itemize}
\item Binary signal transmission over noise channel (cable, radio)
\item Information retrieval
\item airport security scans (is this a weapon or just a hair dryer)
\end{itemize}

The results of one single trial in such an experiment may take one of the values from the following table:

\bigskip

\begin{center}
\begin{tabular}{ c|c|c|}
\centering
  \backslashbox{\small Response}{\small Signal}   & S = yes       & S = no            \\ \hline
  \multicolumn{1}{l|}{R = yes}	                  & Hit           & False Alarm       \\ \hline
  \multicolumn{1}{l|}{R = no}                     & Miss          & Correct Rejection \\ \hline
\end{tabular}
\end{center}
The probabilities to get a certain response given the existence of a signal may look like the following pdf's:

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
  \begin{axis}[axis x line*=bottom, axis y line*=left, enlargelimits=upper, axis on top, domain=0:1, legend style={at={(1.2,0.8)}, anchor=east}]
  \addplot[smooth] file {../data/2014-06-13_dist1.txt};
  \addlegendentry{P(X|S=yes)}
  \addplot[smooth, dashed] file {../data/2014-06-13_dist2.txt};
  \addlegendentry{P(X|S=no)}
  \end{axis}
\end{tikzpicture}

\label{fig:2014-06-13_ex1plot}
\end{figure}

\subsubsection{Response strategy}

We are now looking for a set of values for which our response will be \emph{yes}. This is called the response strategy. It somehow determines from what signal strength onward you would report the signal was there. Formally: \texttt{if} $x\in A$ \texttt{then YES else NO}. Our response strategy should not only depend on the probabilities, but also on the cost of being wrong. The loss function $L(S,R)$ depends now on the costs for the different cases.

\begin{center}
\setlength{\extrarowheight}{.2cm}
\begin{tabular}{ c|c|c|}
  \backslashbox{\small Response}{\small Signal}  &S = yes   &S = no    \\ \hline
  \multicolumn{1}{l|}{R = yes}                   &$C_H$     &$C_{FA}$  \\ \hline
  \multicolumn{1}{l|}{R = no}                    &$C_M$     &$C_{CR}$  \\ \hline
\end{tabular}
\end{center}

The probabilities for Hit and false Alarm are the integrals over the pdf's.

\begin{align*}
P(H)  &= \int_A p(X \mid S=yes)dx = P(R=y \mid S=y) \\
P(FA) &= \int_A p(X \mid S=no)dx  = P(R=y \mid S=n)
\end{align*}

The probabilities for miss and correct response can be directly computed from these.
\begin{align*}
P(M)  &= 1 - P(H)  = P(R=n \mid S=y)\\
P(CR) &= 1 - P(FA) = P(R=n \mid S=n)
\end{align*}

\subsubsection{Minimize expected loss}

As with the proper scoring rules we want to give responses in order to minimize our expected loss for certain costs.
\begin{align*}
E(L(S=y,R)) &= P(H) \cdot C_H    +  \underbrace{P(M)}_{1-P(H)} \cdot C_M \\
E(L(S=n,R)) &= P(FA)\cdot C_{FA} +  \underbrace{P(CR)}_{1-P(FA)}\cdot C_{CR} \\
\end{align*}
In the following we use this shorthand notation:
\begin{align*}
\pi_y &= P(S=y) \\
\pi_n &= P(S=n)
\end{align*}

\begin{align*}
E(L(S,R)) &= \pi_y  E(L(S=y, R)) + \pi_n E(L(S=n, R))\\
&= \pi_y C_H  P(H) + \pi_y C_M - \pi_y C_M P(H) + \pi_n C_{CR} - \pi_n C_{CR} P(FA) + \pi_n C_{FA} P(FA)\\
&= \pi_y P(H) (C_H - C_M) + \pi_n P(FA) (C_{FA} - C_{CR}) + \underbrace{(\pi_y C_M + \pi_n C_{CR})}_{\text{independent of } A} &
\end{align*}

Now we minimize only the parts dependent on $A$.

\begin{align*}
& \pi_y P(H) (C_H - C_M) + \pi_n P(FA) (C_{FA} - C_{CR})\\
&= \pi_y \left(\int_A p(X \mid S=yes) dx\right)(C_H - C_M) + \pi_n \left(\int_A p(X \mid S=no) dx\right) (C_{FA} - C_{CR})\\
&= \int_A \left[\pi_y p(X \mid S=yes)(C_H - C_M) + \pi_n p(X \mid S=no)(C_{FA} - C_{CR}) dx\right] & \\
\end{align*}

Choose $A$ such that we only integrate over the negative part.

\begin{align*}
\pi_y p(X \mid S=yes)(C_H - C_M) + \pi_n p(X \mid S=no)(C_{FA} - C_{CR}) &< 0 \\
\pi_y p(X \mid S=yes)(C_H - C_M) &\leq - \pi_n p(X \mid S=no)(C_{FA} - C_{CR}) \\
\pi_y p(X \mid S=yes)(C_H - C_M) &\leq \pi_n p(X \mid S=no)(C_{CR} - C_{FA}) \\
\end{align*}
\begin{equation*}
\underbrace{\frac{\pi_y p(X \mid S=yes)}{\pi_n p(X \mid S=no)}}_\text{posterior odds} \geq \underbrace{\frac{C_{CR} - C_{FA}}{C_H - C_M}}_\text{costs threshold}
\end{equation*}

Interpretation: We choose $A$ so that the posterior odds are greater then the costs.

Other way of writing:

\begin{equation*}
\underbrace{\frac{p(X \mid S=yes)}{p(X \mid S=no)}}_\text{likelihood ratio} \geq \underbrace{\frac{\pi_n(C_{CR} - C_{FA})}{\pi_y(C_H - C_M)}}_\beta
\end{equation*}


\subsubsection{Use Gaussians for modelling}

We can model the probability of Hits and False Alarms with Gaussians respectively:

\begin{align*}
  P\left(X|s=yes\right) &= \frac{1}{\sqrt{2\pi \sigma^2}} \cdot e^{-\frac{1}{2}\frac{\left(x-\mu_y\right)^2}{\sigma^2}}\\
  P\left(X|s=no\right) &= \frac{1}{\sqrt{2\pi \sigma^2}} \cdot e^{-\frac{1}{2}\frac{\left(x-\mu_n\right)^2}{\sigma^2}}
\end{align*}

This is not easy to calculate, but it is alright to take the $\log$, so we use the log-likelihood ratio and compare that to our previous calculated $\beta$:

\begin{align*}
 -\frac{1}{2\sigma^2} \cdot \left[\left(x-\mu_y\right)^2 - \left(x-\mu_n\right)^2 \right] &\geq log\left(\beta\right)\\
  x^2 - 2x\mu_y + \mu_{y}^{2} - x^2 + 2x\mu_n - \mu_{n}^{2} &\leq -2\sigma^2 \cdot log\left(\beta\right)\\
  2x\left(\mu_n - \mu_y\right)+\mu_{y}^{2}-\mu_{n}^{2}&\leq -2\sigma^2\cdot log\left(\beta\right)\end{align*}
  
By convention the mean of the noise distribution $\mu_n$ is smaller than $\mu_y$, such that by solving for $x$ and dividing by $\left(\mu_n-\mu_y\right)$ the inequality turns and we get:

\begin{align*}
  x  &\geq \underbrace{\frac{-2\sigma^2 \cdot log\left(\beta\right) + \mu_{n}^{2}-\mu_{y}^{2}}{2\left(\mu_n-\mu_y\right)}}_{\Theta}
\end{align*}

That we can interpret such that we answer with yes in the case the x we perceive is bigger than the threshold (criterion) $\Theta$.
In the case of $\beta = 1$ and equal prior probabilities that means:

\begin{align*}
  x \geq \frac{\left(\mu_n-\mu_y\right)\cdot \left(\mu_n+\mu_y\right)}{2\left(\mu_n-\mu_y\right)} = \frac{\left(\mu_n+\mu_y\right)}{2} 
\end{align*}
i.e. the best threshold is just in the middle of the two Gaussians.

\newpage

\subsection{Signal to Noise Ratio}

Now we want to find the limits of perception - how few light can we detect, or in other words: What is a persons signal to noise ratio for light detection. For the case where we again assume two Gaussian distributions with equal deviation, the signal to noise ratio is defined as:

\begin{align*}
SNR = \frac{\mu_y-\mu_n}{\sigma}  
\end{align*} 

As we care for the actual perception (the distance of the Gaussians) of a subject and not his decision-criterion when to say yes, we have to come up with a measure independent of the subjects threshold. Receiver operator characteristics (ROC) allow for that by systematically varying the threshold, such that all possible criteria are covered (from always saying yes, to always saying no). Then we get a curve describing the perception of a subject independently of his criteria.

\bigskip
\begin{center}
\begin{tikzpicture}
  \begin{axis}[axis x line*=bottom, axis y line*=left, enlargelimits=upper, axis on top, domain=0:1, legend style={at={(1.2,0.8)}, anchor=east,xlabel ={P(FA)}, ylabel={P(H)}}]
  \addplot[smooth,domain=0:1, samples=50]{roc(2)};
  \addlegendentry{ROC}
  \end{axis}
\end{tikzpicture}
\end{center}

Varying the threshold may be achieved by the experimenter, by manipulating the costs and pay-offs for False Alarms or Hits. Note that the above depicted graph is a theoretical model. In a real experiment we would get single data points, scattered around such a curve.
 
\end{document}
