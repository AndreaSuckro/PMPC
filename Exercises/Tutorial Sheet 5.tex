% change 0 to the number of the sheet
\def \TutorialSheetNumber{5}
\input{exercises_preamble}

\subsection*{Exercise 1}
\begin{figure}[!ht]
\centering
\begin{tikzpicture}
  \begin{axis}[axis x line*=bottom, axis y line*=left, enlargelimits=upper, axis on top, domain=0:1, legend style={at={(0.98,0.5)}, y post scale=2, x post scale=2, anchor=east}]
  \addplot[smooth, samples=100, color=blue]    {betaDist(1,1)};
    \addlegendentry{$\alpha=1,\beta=1$}
  \addplot[smooth, samples=100, color=purple]  {betaDist(2,2)};
    \addlegendentry{$\alpha=2,\beta=2$}
  \addplot[smooth, samples=100, color=yellow]  {betaDist(3,3)};
    \addlegendentry{$\alpha=3,\beta=3$}
  \addplot[smooth, samples=100, color=red]     {betaDist(1,2)};
    \addlegendentry{$\alpha=1,\beta=2$}
  \addplot[smooth, samples=100, color=magenta] {betaDist(1,3)};
    \addlegendentry{$\alpha=1,\beta=3$}
  \addplot[smooth, samples=100, color=orange]  {betaDist(1,5)};
    \addlegendentry{$\alpha=1,\beta=8$}
  \addplot[smooth, samples=100, color=cyan]    {betaDist(2,1)};
    \addlegendentry{$\alpha=2,\beta=1$}
  \addplot[smooth, samples=100, color=black]   {betaDist(3,1)};
    \addlegendentry{$\alpha=3,\beta=1$}
  \addplot[smooth, samples=100, color=green]   {betaDist(5,1)};
    \addlegendentry{$\alpha=8,\beta=1$}
  \addplot[smooth, samples=100, color=gray]    {betaDist(3,6)};
    \addlegendentry{$\alpha=3,\beta=6$}
  \end{axis}
\end{tikzpicture}
\caption{Different $\beta$-distributions}
\label{fig:2014-05-30_ex1betadistributions}
\end{figure}

We can easily observe that the $\beta$-distribution is symmetric for $\alpha=\beta$ and always 1 in case $\alpha=\beta=1$. If we increase one parameter the distribution's peak wanders to one side, towards 0 for higher $\beta$s and towards 1 for higher $\alpha$s.

If we increase both parameters unequally, the distribution's peak moves towards the higher parameter (similar to the movement mentioned before, see $\alpha=3,\beta=6$) and gets for high values much narrower and steeper (not shown in the plot, see figure \ref{fig:2014-05-30_ex3posterior} for an example).

\bigskip

When using the $\beta$-distribution to learn about the probability $q$ for the thumbtack example we would start with $\alpha=\beta=1$, i.e. uninformed. After gathering some data we can use it to update our prior belief and adjust the parameters according to our posterior distribution. If we continue doing this iteratively the distribution will get closer to the real $q$ of the thumbtack.


\subsection*{Exercise 2}
We have only very limited information and a best guess is that A will win the elections, since there are 53.3\% of the people supporting him. We are not really sure about that because of the lack of information.

Our prior belief can be expressed as a binomial distribution $P(A) = \binom{1000}{533} p^{533}(1-p)^{1000-533}$. However, since the likelihood can be expressed as a Bernoulli distribution ($p^{533}(1-p)^{467}$) we can also use the $\beta$-distribution to model our prior belief: it is conjugate to the Bernoulli distribution. 

We try to find to find $P(A|Poll)$, i.e. the probability that A wins the election given the poll results.
\begin{align*}
P(A|Poll) &= \frac{P(Poll|A)P(A)}{P(Poll)}\\
P(A|Poll) &= \frac{p^{533}(1-p)^{467} \cdot p^{\alpha-1}(1-p)^{\beta-1}}{B(\alpha,\beta)}\\
\text{using $\alpha=533$ and $\beta=467$:}\\
P(A|Poll) &= \frac{p^{533}(1-p)^{467} \cdot p^{533-1}(1-p)^{467-1}}{B(\alpha_n,\beta_n)}\\
P(A|Poll) &= \frac{p^{1065}(1-p)^{933}}{B(1066,934)}
\end{align*}

We can plot this posterior belief $P(A|Poll)$ (figure \ref{fig:2014-05-30_ex2pdf}) and see how likely it is that A wins.
\begin{figure}[!ht]
\centering
\begin{tikzpicture}
  \begin{axis}[axis x line*=bottom, axis y line*=left, enlargelimits=upper, axis on top, domain=0:1, legend style={at={(0.98,0.98)}, anchor=east}]
  % csvwrite('beta.txt',[[0:0.001:1]' betapdf(0:0.001:1,1066,934)'])
  \addplot[smooth] file {../Script/data/2014-05-30_exercise2_pdf.txt};
    \addlegendentry{$\alpha=1066,\beta=934$}
  \end{axis}
\end{tikzpicture}
\caption{How likely is it, that A wins the election?}
\label{fig:2014-05-30_ex2pdf}
\end{figure}


\subsection*{Exercise 3}
We denote $B$ as the number of people believing in UFOs and $D$ as the data (i.e. the poll results) given.

Using the $\beta$-Distribution we come up with the following model:
\begin{align*}
                P(B|D) &= \frac{P(D|B)P(B)}{P(D)} \\
\Leftrightarrow P(B|D) &= \frac{q^{230}\left(1-q\right)^{500-230}P(B)}{P(D)}\\
\Leftrightarrow P(B|D) &= \frac{\overbrace{q^{230}\left(1-q\right)^{270}}^\text{Bernoulli} \overbrace{q^{\alpha-1}\left(1-q\right)^{\beta-1}}^\text{$\beta$-Distribution}}{B(\alpha_n,\beta_n)} \\
\Leftrightarrow P(B|D) &= \frac{q^{230}\left(1-q\right)^{270} q^{1-1}\left(1-q\right)^{9-1}}{B(\alpha_n,\beta_n)} \\
\Leftrightarrow P(B|D) &= \frac{q^{231-1}\left(1-q\right)^{279-1}}{B(231,279)} \\
\Leftrightarrow P(B|D) &= \frac{q^{230}\left(1-q\right)^{278}}{B(231,279)}
\end{align*}

The prior distribution (figure \ref{fig:2014-05-30_ex3prior}) is far to the left, as we have a relatively huge $\beta$ compared to $\alpha$.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
  \begin{axis}[axis x line*=bottom, axis y line*=left, enlargelimits=upper, axis on top, domain=0:1]
  \plot[smooth, color=blue] file {../Script/data/2014-05-30_priorPDF_data.txt};
    \addlegendentry{$\alpha=1,\beta=9$}
  \end{axis}
\end{tikzpicture}
\caption{Prior $\beta$-Distribution}
\label{fig:2014-05-30_ex3prior}
\end{figure}

With the new $\alpha = 231$ and $\beta = 279$ we can find the $95\%$ interval by calculating the CDF of the posterior $\beta$-distribution and searching for the x-values for $y_{min}=0.025$ and $y_{max}=0.975$. Using these x-values for the corresponding PDF yields the $95\%$ interval (figure \ref{fig:2014-05-30_ex3posterior}).

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
  \begin{axis}[axis x line*=bottom, axis y line*=left, enlargelimits=upper, axis on top, domain=0:1, name=cdf, legend style={at={(0.98,0.5)}, anchor=east}]
  \plot[smooth, color=blue] file {../Script/data/2014-05-30_posteriorCDF_data.txt};
    \addlegendentry{$\alpha=231,\beta=279$}
  \plot[color=red] {0.975};
    \addlegendentry{$x=0.975$}
  \plot[color=red] {0.025};
    \addlegendentry{$x=0.025$}
  \node at (axis cs:0.41,0.025) {$\times$};
  \node at (axis cs:0.496,0.975) {$\times$};
  \end{axis}
  
  \begin{axis}[axis x line*=bottom, axis y line*=left, enlargelimits=upper, axis on top, domain=0:1, name=pdf, at=(cdf.right of east), anchor=left of west]
  \plot+[domain=0.41:0.496, samples=1000, pattern=flexible hatch, mark=none
            hatch distance=0.2pt, hatch thickness=1pt,
            draw=blue, pattern color=cyan, area legend]
            file {../Script/data/2014-05-30_posteriorPDF_area_data.txt} \closedcycle;
    \addlegendentry{$95\%$}
  \plot[smooth, color=red] file {../Script/data/2014-05-30_posteriorPDF_data.txt};
    \addlegendentry{$\alpha=231,\beta=279$}
  \end{axis}
\end{tikzpicture}
\caption{Left: CDF and 95\% intersections. Right: PDF and 95\% interval.}
\label{fig:2014-05-30_ex3posterior}
\end{figure}

\bigskip

\texttt{MATLAB} code to try out exercise 3:
\lstinputlisting{../Script/data/2014-05-30_posteriorBetaDistribution.m}


\subsection*{Exercise 4}
We would like to know what $p(q)$ is. As frequentists we can't do that, we need a Bayesian approach. So we start with the null hypothesis $H_0$: Subjects can't discriminate: $q = \frac{1}{2}, n = 25$. Where $q$ denotes the discrimination factor for the subjects and $n$ is the number of trials. We now measure H (\# ``Heads'': correct discriminations).
\begin{align*}
P(H=h|q) = \underbrace{ \binom{n}{h} \overbrace{q^h (1-q)^{n-h}}^\text{Bernoulli} }_\text{Binomial distribution}
\end{align*}

Subjects can discriminate if they get >20 correct answers. In that case we reject the null hypothesis.

\bigskip

If the subject cannot distinguish them, it will most likely be right in about 50 \% of the cases.

How often will it happen that the subject gets more than 20 correct responses although it cannot distinguish them at all (i.e. the experimenter makes a type II ($\beta$-) error)?

\begin{align*}
p(q>\frac{1}{2}|H>20,n=25) &= \frac{P(H>20,n=25|q>\frac{1}{2})P(q>\frac{1}{2})}{P(H>20,n=25)} \\
p(q>\frac{1}{2}|H>20,n=25) &= \frac{ \binom{25}{5} p^{25}(1-p)^{25-5}p^{25-1}(1-p)^{25-5-1}}{B(\alpha_n,\beta_n)} \\
p(q>\frac{1}{2}|H>20,n=25) &= \frac{ \binom{25}{5} p^{50-1}(1-p)^{40-1}}{B(50,40)} \\
p(q>\frac{1}{2}|H>20,n=25) &= \frac{ \binom{25}{5} p^{49}(1-p)^{39}}{B(50,40)}
\end{align*}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
  \begin{axis}[axis x line*=bottom, axis y line*=left, enlargelimits=upper, axis on top, domain=0:1, legend style={at={(0.98,0.5)}, anchor=east}]
  % csvwrite('beta.txt',[[0:0.001:1]' (nchoosek(25,5) * (betapdf(0:0.001:1,50,40))'])
  \plot[smooth, color=red] file {../Script/data/2014-05-30_exercise4_pdf.txt};
    \addlegendentry{$\alpha=50,\beta=40$}
  \end{axis}
\end{tikzpicture}
\caption{PDF for $q>\frac{1}{2}$}
\label{fig:2014-05-30_ex4pdf}
\end{figure}

After 1000 iterations we get the following result for our posterior:
\begin{figure}[!ht]
\centering
\begin{tikzpicture}
  \begin{axis}[axis x line*=bottom, axis y line*=left, enlargelimits=upper, axis on top, domain=0:1, legend style={at={(0.98,0.5)}, anchor=east}]
  % csvwrite('beta.txt',[[0:0.001:1]' (nchoosek(25,5) * (betapdf(0:0.001:1,50,40))'])
  \plot[smooth, color=red] file {../Script/data/2014-05-30_exercise4_pdf_sim.txt};
    \addlegendentry{$\alpha=25001,\beta=20001$}
  \end{axis}
\end{tikzpicture}
\caption{1000 iterations}
\label{fig:2014-05-30_ex4pdfsim}
\end{figure}


\subsection*{Exercise 5}
\begin{enumerate}
	\item Since $p < 0.01$ it's not absolutely disproved.
  \item You don't find the probability of the $H_0$ ($p(q=\frac{1}{2}|H=h,n=25)$), but instead you find $p(H>20|q=\frac{1}{2},n=25)$ - the p-value. But the p-value is not sufficient to know the probability of $H_0$.
  \item As in 1: it's not certain.
  \item Finding $p(q>\frac{1}{2}|H=h,n=25)$ is only possible with Bayes' rule, so it's impossible from a frequentist point of view.
  \item To know the probability that the decision taken was wrong you need to know the type II ($\beta$-) error. This is not possible if you don't know $q$, which you don't in a NHST.
  \item It's not possible to know what happens if you repeat an experiment. The results might be similar or totally different - so they are definitely not significant with a probability of 99\%.
  \item This is the only true fact in this list that we can derive from a NHST.
\end{enumerate}

\subsection*{Exercise 6}


\subsection*{Exercise 8}
\begin{align*}
E(f(X)) &= \sum_{x\in\Omega}p(x)f(x) \\
E(a\cdot f(X) + b) &= \sum_{x\in\Omega}ap(x)f(x)+b = b+a\sum_{x\in\Omega}p(x)f(x) = b+a\cdot E(f(X))\\
                   &= a\cdot E(f(x)) + b
\end{align*}


\end{document}